{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import math\n",
    "from evaluation_metrics import entropy\n",
    "\n",
    "import sys\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from copy import deepcopy\n",
    "from typing import Any, List, Optional, Union\n",
    "from multiprocessing import Manager, Process, Queue, cpu_count\n",
    "\n",
    "from util import downsize\n",
    "from evaluation_metrics import metrics, cleanup\n",
    "from description import Description\n",
    "from subgroup import Subgroup\n",
    "from beam import Beam\n",
    "from workers import evaluate_subgroups, beam_adder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GENERATE DATA ##\n",
    "datasize = 2000\n",
    "randomvariables = 9\n",
    "## GENERATE DATA ##\n",
    "\n",
    "predictor = list(np.random.normal(10,1,datasize))\n",
    "errorsd = 0.5\n",
    "result = []\n",
    "\n",
    "# we doctor our variables in a non random way\n",
    "v1 = [0,1,0,1,0]*int(datasize/5)\n",
    "v2 = [0]*int(datasize/5*3) + [1]*int(datasize/5*2)\n",
    "\n",
    "\n",
    "noisevars = [list(np.random.binomial(1,0.4,datasize)) for _ in range(randomvariables-2)]\n",
    "\n",
    "variables = [v1, v2] +noisevars\n",
    "# generate result;\n",
    "# result data where first two variables are both 1 is different\n",
    "for i in range(datasize):\n",
    "    if variables[0][i] == 1 and variables[1][i] == 1:\n",
    "        result.append(16* predictor[i]  + np.random.normal(0,errorsd) )\n",
    "    # elif variables[0][i] == 1 and variables[2][i] == 1:\n",
    "    #     result.append(4 * predictor[i]   + np.random.normal(0,errorsd) )\n",
    "    else:\n",
    "        result.append(10* predictor[i]  + np.random.normal(0,errorsd) )\n",
    "\n",
    "\n",
    "# create a dataframe with number i as column title with the before generated columns\n",
    "df = pd.DataFrame({i:ls for i,ls in enumerate(variables)})\n",
    "\n",
    "df['result'] = result\n",
    "df['predictor'] = predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # JACCARD EMM INIT\n",
    "# width=20\n",
    "# depth=2\n",
    "# evaluation_metric='regression'\n",
    "# n_jobs=-1\n",
    "# log_level=1\n",
    "\n",
    "# #defaults:\n",
    "# strategy = 'maximize'\n",
    "# n_bins = 10\n",
    "# bin_strategy = 'equidepth'\n",
    "# candidate_size = None\n",
    "# log_level=50\n",
    "\n",
    "# depth = depth\n",
    "# evaluation_metric = evaluation_metric\n",
    "# if n_jobs == -1:\n",
    "#     n_jobs = cpu_count()\n",
    "# else:\n",
    "#     n_jobs = min(n_jobs, cpu_count())\n",
    "# if hasattr(evaluation_metric, '__call__'):\n",
    "#     evaluation_function = evaluation_metric\n",
    "# else:\n",
    "#     try:\n",
    "#         evaluation_function = metrics[evaluation_metric]\n",
    "#     except KeyError:\n",
    "#         raise ValueError(f\"Nu such metric: {evaluation_metric}\")\n",
    "# settings = dict(\n",
    "#     strategy=strategy,\n",
    "#     width=width,\n",
    "#     n_bins=n_bins,\n",
    "#     bin_strategy=bin_strategy,\n",
    "#     candidate_size=candidate_size\n",
    "# )\n",
    "# beam = None\n",
    "# target_columns = None\n",
    "# dataset_target = None\n",
    "# dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMM INIT\n",
    "class EMM():\n",
    "    def __init__(self, width, depth, evaluation_metric='regression', strategy = 'maximize',\n",
    "                 n_bins = 10, bin_strategy = 'equidepth', candidate_size = None, log_level=1 ) -> None:\n",
    "        \"\"\"Initialization for the beam search exceptional model mining procedure\"\"\"\n",
    "        logging.basicConfig(filename=None, level=log_level, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "        # removed the n_jobs from code, because we removed the multicore processing\n",
    "        if hasattr(evaluation_metric, '__call__'):\n",
    "            self.evaluation_function = evaluation_metric\n",
    "        else:\n",
    "            try:\n",
    "                self.evaluation_function = metrics[evaluation_metric]\n",
    "            except KeyError:\n",
    "                raise ValueError(f\"Nu such metric: {evaluation_metric}\")\n",
    "        self.settings = dict(\n",
    "            strategy=strategy,\n",
    "            width=width,\n",
    "            n_bins=n_bins,\n",
    "            bin_strategy=bin_strategy,\n",
    "            candidate_size=candidate_size\n",
    "        )\n",
    "        self.beam = None\n",
    "        self.target_columns = None\n",
    "        self.dataset_target = None\n",
    "        self.dataset = None\n",
    "        self.depth = depth\n",
    "    \n",
    "    def set_data(self, data:pd.Dataframe, target_cols):\n",
    "        \"\"\"This method takes a dataset and prepares it for the beam search\"\"\"\n",
    "        logging.info(\"Start\")\n",
    "        self.data, translations = downsize(deepcopy(data))\n",
    "        self.settings['object_cols'] = translations\n",
    "        dataset = Subgroup(data, Description('all'))\n",
    "        _, dataset.target = self.evaluation_function(data[target_cols], data[target_cols])\n",
    "        self.regressioncache = dataset.target\n",
    "        self.beam = Beam(dataset, self.settings)\n",
    "        target_cols = list(target_cols,)\n",
    "        descriptive_cols = [c for c in data.columns if c not in target_cols]\n",
    "        if any(c not in data.columns for c in descriptive_cols + target_cols):\n",
    "            raise ValueError(\"All specified columns should be present in the dataset\")\n",
    "        self.dataset_target = data[target_cols]\n",
    "        self.target_columns = target_cols\n",
    "    \n",
    "    def subgroupify(self):\n",
    "        subgroups = []\n",
    "        for subgroup in self.beam.subgroups:\n",
    "                for col in self.descriptive_cols:\n",
    "                    newgroups = create_subgroup_lists(subgroup, col, self.settings)\n",
    "                    subgroups = subgroups + newgroups\n",
    "        self.candidates = subgroups\n",
    "    \n",
    "    def calc_score(self, p = True):\n",
    "        for candidate in self.candidates:\n",
    "            candidate_target = candidate.data[self.target_columns]\n",
    "            candidate.score, candidate.target = regression(candidate_target, self.dataset_target, comparecache=self.regressioncache)\n",
    "            self.beam.add(candidate)\n",
    "        self.beam.select_cover_based()\n",
    "        if p== True:\n",
    "            self.beam.print()\n",
    "        else:\n",
    "            logging.info(\"finished an iteration\")\n",
    "    \n",
    "    def increase_depth(self,iterations = 1):\n",
    "        for _ in range(iterations):\n",
    "            self.subgroupify()\n",
    "            self.calc_score(p=False)\n",
    "        self.beam.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Subgroup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m     N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset_target)\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mn\u001b[38;5;241m/\u001b[39mN \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(n\u001b[38;5;241m/\u001b[39mN) \u001b[38;5;241m-\u001b[39m n_c\u001b[38;5;241m/\u001b[39mN \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(n_c\u001b[38;5;241m/\u001b[39mN)\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_subgroup_lists\u001b[39m(subgroup: \u001b[43mSubgroup\u001b[49m, column: \u001b[38;5;28mstr\u001b[39m, settings: \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;124;03m\"\"\"This function takes a subgroup and column and makes all the possible subgroup splits on that column\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124;03m    This is then returned as a list\"\"\"\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     resultinggroups \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Subgroup' is not defined"
     ]
    }
   ],
   "source": [
    "def regression(subgroup_target, dataset_target, comparecache, use_complement=False):\n",
    "    if len(subgroup_target) < 20: # less than 20 rows is not enough to build on.\n",
    "        return 0, None\n",
    "    if len(subgroup_target.columns) != 2:\n",
    "        raise ValueError(\"Correlation metric expects exactly 2 columns as target variables\")\n",
    "    x_col, y_col = list(subgroup_target.columns)\n",
    "    est = sm.OLS(subgroup_target[y_col], subgroup_target[x_col])\n",
    "    est = est.fit()\n",
    "    coef = est.summary2().tables[1]['Coef.'][x_col]\n",
    "    p = est.summary2().tables[1]['P>|t|'][x_col]\n",
    "    if math.isnan(p):\n",
    "        return 0, 0\n",
    "    if (1 - p) < 0.99:\n",
    "        return 0, 0\n",
    "    return entropy(subgroup_target, dataset_target) * abs(coef - comparecache), coef\n",
    "\n",
    "def entropy(subgroup_target, dataset_target):\n",
    "    n_c = max(1, len(dataset_target) - len(subgroup_target))\n",
    "    n = len(subgroup_target)\n",
    "    N = len(dataset_target)\n",
    "    return -n/N * math.log(n/N) - n_c/N * math.log(n_c/N)\n",
    "\n",
    "def create_subgroup_lists(subgroup: Subgroup, column: str, settings: dict):\n",
    "    \"\"\"This function takes a subgroup and column and makes all the possible subgroup splits on that column\n",
    "    This is then returned as a list\"\"\"\n",
    "    resultinggroups = []\n",
    "    if column in subgroup.description:\n",
    "        return []\n",
    "    data = subgroup.data\n",
    "    values = list(data[column].unique())\n",
    "    if len(values) == 1:  # No need to make a split for a single value\n",
    "        return []\n",
    "    if column in settings['object_cols'] or len(values) < settings['n_bins']:\n",
    "        while len(values) > 0:\n",
    "            value = values.pop(0)\n",
    "            subset = data[data[column] == value]\n",
    "            resultinggroups.append( Subgroup(subset, deepcopy(subgroup.description).extend(column, value)))\n",
    "    else:  # Float or Int\n",
    "        if settings['bin_strategy'] == 'equidepth':\n",
    "            _, intervals = pd.qcut(data[column].tolist(), q=min(settings['n_bins'], len(values)),\n",
    "                                   duplicates='drop', retbins=True)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid bin strategy `{settings['strategy']}`\")\n",
    "        intervals = list(intervals)\n",
    "        lower_bound = intervals.pop(0)\n",
    "        while len(intervals) > 0:\n",
    "                upper_bound = intervals.pop(0)\n",
    "                subset = data[(data[column] > lower_bound) & (data[column] <= upper_bound)]\n",
    "                resultinggroups.append( Subgroup(subset, deepcopy(subgroup.description).extend(column, [lower_bound, upper_bound])) )\n",
    "                lower_bound = upper_bound\n",
    "    return resultinggroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-17 16:30:58,125 - INFO - Start\n",
      "2024-10-17 16:30:58,134 - INFO - Memory usage before downsizing 117.31 MB\n",
      "2024-10-17 16:30:58,139 - INFO - Memory usage after downsizing 33.33 MB\n"
     ]
    }
   ],
   "source": [
    "# SETTING DATA\n",
    "target_cols = ['result','predictor']\n",
    "data = df\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making subgroups done\n"
     ]
    }
   ],
   "source": [
    "print('making subgroups', end='')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-17 16:30:58,385 - DEBUG - --------------------\n",
      "2024-10-17 16:30:58,386 - DEBUG - 1 = 0 0.008291245956183543 (1200)\n",
      "2024-10-17 16:30:58,387 - DEBUG - 0 = 0 0.008274994520212456 (1200)\n",
      "2024-10-17 16:30:58,387 - DEBUG - 0 = 1 0.00769718852873312 (800)\n",
      "2024-10-17 16:30:58,388 - DEBUG - 1 = 1 0.007609853983744397 (800)\n",
      "2024-10-17 16:30:58,388 - DEBUG - 3 = 1 0.0006933671099866324 (786)\n",
      "2024-10-17 16:30:58,389 - DEBUG - 2 = 1 0.0006109023041132762 (812)\n",
      "2024-10-17 16:30:58,389 - DEBUG - 5 = 1 0.0005381518640466143 (804)\n",
      "2024-10-17 16:30:58,390 - DEBUG - 3 = 0 0.00046516364092260074 (1214)\n",
      "2024-10-17 16:30:58,390 - DEBUG - 7 = 1 0.0004419369443377553 (790)\n",
      "2024-10-17 16:30:58,391 - DEBUG - 2 = 0 0.0004255474017664676 (1188)\n",
      "2024-10-17 16:30:58,391 - DEBUG - 5 = 0 0.00035221473462178614 (1196)\n",
      "2024-10-17 16:30:58,391 - DEBUG - 7 = 0 0.0002867031434434397 (1210)\n",
      "2024-10-17 16:30:58,392 - DEBUG - 4 = 1 0.0001686347946487329 (780)\n",
      "2024-10-17 16:30:58,392 - DEBUG - 4 = 0 0.00010609368244808597 (1220)\n",
      "2024-10-17 16:30:58,393 - DEBUG - 6 = 1 7.455810772198877e-05 (808)\n",
      "2024-10-17 16:30:58,393 - DEBUG - 6 = 0 5.015105830187434e-05 (1192)\n",
      "2024-10-17 16:30:58,394 - DEBUG - 8 = 1 2.2984900353443406e-06 (782)\n",
      "2024-10-17 16:30:58,394 - DEBUG - 8 = 0 1.4558765516714696e-06 (1218)\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE and add\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making subgroups done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-17 16:31:00,740 - DEBUG - --------------------\n",
      "2024-10-17 16:31:00,740 - DEBUG - 0 = 1 AND 1 = 1 0.011073179339469363 (320)\n",
      "2024-10-17 16:31:00,740 - DEBUG - 0 = 0 AND 2 = 0 0.008125899369548757 (748)\n",
      "2024-10-17 16:31:00,740 - DEBUG - 1 = 0 AND 8 = 0 0.00811624858535081 (737)\n",
      "2024-10-17 16:31:00,740 - DEBUG - 1 = 0 AND 3 = 0 0.00810906525765092 (738)\n",
      "2024-10-17 16:31:00,740 - DEBUG - 0 = 0 AND 4 = 0 0.00808145088910405 (735)\n",
      "2024-10-17 16:31:00,740 - DEBUG - 1 = 0 AND 7 = 0 0.008099322650937086 (733)\n",
      "2024-10-17 16:31:00,740 - DEBUG - 0 = 0 AND 8 = 0 0.00806677039035524 (730)\n",
      "2024-10-17 16:31:00,740 - DEBUG - 0 = 0 AND 3 = 0 0.008059772180217724 (726)\n",
      "2024-10-17 16:31:00,740 - DEBUG - 1 = 0 AND 5 = 0 0.008060981183114926 (720)\n",
      "2024-10-17 16:31:00,748 - DEBUG - 1 = 0 AND 4 = 0 0.00805223528804073 (721)\n",
      "2024-10-17 16:31:00,748 - DEBUG - 1 = 0 AND 2 = 0 0.008009663592855602 (707)\n",
      "2024-10-17 16:31:00,748 - DEBUG - 0 = 0 AND 7 = 0 0.007988196094674434 (704)\n",
      "2024-10-17 16:31:00,748 - DEBUG - 1 = 0 AND 0 = 0 0.008046366411653658 (720)\n",
      "2024-10-17 16:31:00,748 - DEBUG - 1 = 0 AND 6 = 1 0.006914009067565443 (499)\n",
      "2024-10-17 16:31:00,748 - DEBUG - 0 = 0 AND 7 = 1 0.006872203939034092 (496)\n",
      "2024-10-17 16:31:00,748 - DEBUG - 1 = 0 AND 6 = 0 0.007985805514709492 (701)\n",
      "2024-10-17 16:31:00,748 - DEBUG - 0 = 0 AND 5 = 0 0.008007469730162534 (710)\n",
      "2024-10-17 16:31:00,748 - DEBUG - 0 = 0 AND 6 = 0 0.0080376228641705 (720)\n",
      "2024-10-17 16:31:00,748 - DEBUG - 1 = 1 AND 0 = 1 0.011073179339469363 (320)\n",
      "2024-10-17 16:31:00,748 - DEBUG - 2 = 0 AND 0 = 0 0.008125899369548757 (748)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "subgroups = []\n",
    "for subgroup in beam.subgroups:\n",
    "        for col in descriptive_cols:\n",
    "            newgroups = create_subgroup_lists(subgroup, col, settings)\n",
    "            subgroups = subgroups + newgroups\n",
    "# EVALUATE and add\n",
    "for subgroup in subgroups:\n",
    "    subgroup_target = subgroup.data[target_columns]\n",
    "    subgroup.score, subgroup.target = regression(subgroup_target, dataset_target, comparecache=regressioncache)\n",
    "    beam.add(subgroup)\n",
    "beam.select_cover_based()\n",
    "beam.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-17 16:31:00,758 - DEBUG - --------------------\n",
      "2024-10-17 16:31:00,762 - DEBUG - 0 = 1 AND 1 = 1 0.011073179339469363 (320)\n",
      "2024-10-17 16:31:00,762 - DEBUG - 0 = 0 AND 2 = 0 0.008125899369548757 (748)\n",
      "2024-10-17 16:31:00,763 - DEBUG - 1 = 0 AND 8 = 0 0.00811624858535081 (737)\n",
      "2024-10-17 16:31:00,763 - DEBUG - 1 = 0 AND 3 = 0 0.00810906525765092 (738)\n",
      "2024-10-17 16:31:00,764 - DEBUG - 0 = 0 AND 4 = 0 0.00808145088910405 (735)\n",
      "2024-10-17 16:31:00,764 - DEBUG - 1 = 0 AND 7 = 0 0.008099322650937086 (733)\n",
      "2024-10-17 16:31:00,765 - DEBUG - 0 = 0 AND 8 = 0 0.00806677039035524 (730)\n",
      "2024-10-17 16:31:00,765 - DEBUG - 0 = 0 AND 3 = 0 0.008059772180217724 (726)\n",
      "2024-10-17 16:31:00,766 - DEBUG - 1 = 0 AND 5 = 0 0.008060981183114926 (720)\n",
      "2024-10-17 16:31:00,767 - DEBUG - 1 = 0 AND 4 = 0 0.00805223528804073 (721)\n",
      "2024-10-17 16:31:00,767 - DEBUG - 1 = 0 AND 2 = 0 0.008009663592855602 (707)\n",
      "2024-10-17 16:31:00,768 - DEBUG - 0 = 0 AND 7 = 0 0.007988196094674434 (704)\n",
      "2024-10-17 16:31:00,769 - DEBUG - 1 = 0 AND 0 = 0 0.008046366411653658 (720)\n",
      "2024-10-17 16:31:00,770 - DEBUG - 1 = 0 AND 6 = 1 0.006914009067565443 (499)\n",
      "2024-10-17 16:31:00,770 - DEBUG - 0 = 0 AND 7 = 1 0.006872203939034092 (496)\n",
      "2024-10-17 16:31:00,770 - DEBUG - 1 = 0 AND 6 = 0 0.007985805514709492 (701)\n",
      "2024-10-17 16:31:00,771 - DEBUG - 0 = 0 AND 5 = 0 0.008007469730162534 (710)\n",
      "2024-10-17 16:31:00,771 - DEBUG - 0 = 0 AND 6 = 0 0.0080376228641705 (720)\n",
      "2024-10-17 16:31:00,772 - DEBUG - 1 = 1 AND 0 = 1 0.011073179339469363 (320)\n",
      "2024-10-17 16:31:00,773 - DEBUG - 2 = 0 AND 0 = 0 0.008125899369548757 (748)\n"
     ]
    }
   ],
   "source": [
    "beam.decrypt_descriptions(translations)\n",
    "beam.print()\n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
